{"nbformat_minor": 2, "cells": [{"source": "# What's in this exercise\nBasics of how to work with Azure Cosmos DB from Jupyter <B>in batch</B>.<BR>\nSection 03: Create operation (Crud)<BR>", "cell_type": "markdown", "metadata": {}}, {"source": "### Prerequisites\nThe Datastax connector for Cassandra requires the Azure Comsos DB Cassandra API connection details to be initialized as part of the spark context.  When you launch a Jupyter notebook, the spark session and context are already initialized and it is not advisable to stop and reinitialize the Spark context unless with every configuration set as part of the HDInsight default Jupyter notebook start-up.  One workaround is to add the Cassandra instance details to Ambari, Spark2 service configuration directly.  This is a one-time activity that requires a Spark2 service restart.<BR>\n\n1.  Go to Ambari, Spark2 service and click on configs\n2.  Then go to custom spark2-defaults and add a new property with the following, and restart Spark2 service:\nspark.cassandra.connection.host=YOUR_COSMOSDB_ACCOUNT_NAME.cassandra.cosmosdb.azure.com<br>\nspark.cassandra.connection.port=10350<br>\nspark.cassandra.connection.ssl.enabled=true<br>\nspark.cassandra.auth.username=YOUR_COSMOSDB_ACCOUNT_NAME<br>\nspark.cassandra.auth.password=YOUR_COSMOSDB_KEY<br>", "cell_type": "markdown", "metadata": {}}, {"source": "---------\n## 1.0. Cassandra API connection", "cell_type": "markdown", "metadata": {}}, {"source": "### 1.0.1. Configure dependencies", "cell_type": "markdown", "metadata": {}}, {"execution_count": 1, "cell_type": "code", "source": "%%configure -f\n{ \"conf\": {\"spark.jars.packages\": \"com.datastax.spark:spark-cassandra-connector_2.11:2.3.0,com.microsoft.azure.cosmosdb:azure-cosmos-cassandra-spark-helper:1.0.0\" }}", "outputs": [{"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "Current session configs: <tt>{u'kind': 'spark', u'conf': {u'spark.jars.packages': u'com.datastax.spark:spark-cassandra-connector_2.11:2.3.0,com.microsoft.azure.cosmosdb:azure-cosmos-cassandra-spark-helper:1.0.0'}}</tt><br>"}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>13</td><td>application_1536862968089_0017</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn1-bhoomi.fy0cecrwzcoefky0ef5errkw5g.cx.internal.cloudapp.net:8088/proxy/application_1536862968089_0017/\">Link</a></td><td><a target=\"_blank\" href=\"http://wn0-bhoomi.fy0cecrwzcoefky0ef5errkw5g.cx.internal.cloudapp.net:30060/node/containerlogs/container_e02_1536862968089_0017_01_000001/livy\">Link</a></td><td></td></tr></table>"}, "metadata": {}}], "metadata": {"collapsed": false}}, {"source": "### 1.0.2. Cassandra API configuration", "cell_type": "markdown", "metadata": {}}, {"execution_count": 2, "cell_type": "code", "source": "import org.apache.spark.rdd.RDD\nimport org.apache.spark.{SparkConf, SparkContext}\n\nimport spark.implicits._\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.Column\nimport org.apache.spark.sql.types.{StructType, StructField, StringType, IntegerType,LongType,FloatType,DoubleType, TimestampType}\nimport org.apache.spark.sql.cassandra._\n\n//datastax Spark connector\nimport com.datastax.spark.connector._\nimport com.datastax.spark.connector.cql.CassandraConnector\nimport com.datastax.driver.core.{ConsistencyLevel, DataType}\nimport com.datastax.spark.connector.writer.WriteConf\n\n//Azure Cosmos DB library for multiple retry\nimport com.microsoft.azure.cosmosdb.cassandra\n\n// Specify connection factory for Cassandra\nspark.conf.set(\"spark.cassandra.connection.factory\", \"com.microsoft.azure.cosmosdb.cassandra.CosmosDbConnectionFactory\")\n\n// Parallelism and throughput configs\nspark.conf.set(\"spark.cassandra.output.batch.size.rows\", \"1\")\nspark.conf.set(\"spark.cassandra.connection.connections_per_executor_max\", \"10\")\nspark.conf.set(\"spark.cassandra.output.concurrent.writes\", \"100\")\nspark.conf.set(\"spark.cassandra.concurrent.reads\", \"512\")\nspark.conf.set(\"spark.cassandra.output.batch.grouping.buffer.size\", \"1000\")\nspark.conf.set(\"spark.cassandra.connection.keep_alive_ms\", \"60000000\") //Increase this number as needed\nspark.conf.set(\"spark.cassandra.output.ignoreNulls\",\"true\")", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Starting Spark application\n"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>17</td><td>application_1536862968089_0021</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn1-bhoomi.fy0cecrwzcoefky0ef5errkw5g.cx.internal.cloudapp.net:8088/proxy/application_1536862968089_0021/\">Link</a></td><td><a target=\"_blank\" href=\"http://wn3-bhoomi.fy0cecrwzcoefky0ef5errkw5g.cx.internal.cloudapp.net:30060/node/containerlogs/container_e02_1536862968089_0021_01_000001/livy\">Link</a></td><td>\u2714</td></tr></table>"}, "metadata": {}}, {"output_type": "stream", "name": "stdout", "text": "SparkSession available as 'spark'.\n"}], "metadata": {"collapsed": false}}, {"source": "---\n## 2.0. Dataframe API", "cell_type": "markdown", "metadata": {}}, {"execution_count": 3, "cell_type": "code", "source": "//Delete data from prior runs\nval cdbConnector = CassandraConnector(sc)\ncdbConnector.withSessionDo(session => session.execute(\"delete from books_ks.books where book_id in ('b00300','b00001','b00023','b00501','b09999','b01001','b00999','b03999','b02999','b000009');\"))", "outputs": [{"output_type": "stream", "name": "stdout", "text": "res19: com.datastax.driver.core.ResultSet = ResultSet[ exhausted: true, Columns[]]"}], "metadata": {"collapsed": false}}, {"source": "### 2.0.1. Create a dataframe", "cell_type": "markdown", "metadata": {}}, {"execution_count": 4, "cell_type": "code", "source": "val booksDF = Seq(\n   (\"b00001\", \"Arthur Conan Doyle\", \"A study in scarlet\", 1887),\n   (\"b00023\", \"Arthur Conan Doyle\", \"A sign of four\", 1890),\n   (\"b01001\", \"Arthur Conan Doyle\", \"The adventures of Sherlock Holmes\", 1892),\n   (\"b00501\", \"Arthur Conan Doyle\", \"The memoirs of Sherlock Holmes\", 1893),\n   (\"b00300\", \"Arthur Conan Doyle\", \"The hounds of Baskerville\", 1901)\n).toDF(\"book_id\", \"book_author\", \"book_name\", \"book_pub_year\")\n\nbooksDF.printSchema", "outputs": [{"output_type": "stream", "name": "stdout", "text": "root\n |-- book_id: string (nullable = true)\n |-- book_author: string (nullable = true)\n |-- book_name: string (nullable = true)\n |-- book_pub_year: integer (nullable = false)"}], "metadata": {"collapsed": false}}, {"execution_count": 5, "cell_type": "code", "source": "booksDF.show", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+-------+------------------+--------------------+-------------+\n|book_id|       book_author|           book_name|book_pub_year|\n+-------+------------------+--------------------+-------------+\n| b00001|Arthur Conan Doyle|  A study in scarlet|         1887|\n| b00023|Arthur Conan Doyle|      A sign of four|         1890|\n| b01001|Arthur Conan Doyle|The adventures of...|         1892|\n| b00501|Arthur Conan Doyle|The memoirs of Sh...|         1893|\n| b00300|Arthur Conan Doyle|The hounds of Bas...|         1901|\n+-------+------------------+--------------------+-------------+"}], "metadata": {"collapsed": false}}, {"source": "### 2.0.2. Persist to Azure Cosmos DB Cassandra API", "cell_type": "markdown", "metadata": {}}, {"execution_count": 6, "cell_type": "code", "source": "//Set individual records to strong consistency and ttl to specific value\nbooksDF.write.mode(\"append\").format(\"org.apache.spark.sql.cassandra\").options(Map( \"table\" -> \"books\", \"keyspace\" -> \"books_ks\", \"output.consistency.level\" -> \"ALL\", \"ttl\" -> \"10000000\")).save()", "outputs": [], "metadata": {"collapsed": false}}, {"source": "---\n## 3.0. RDD API", "cell_type": "markdown", "metadata": {}}, {"execution_count": 7, "cell_type": "code", "source": "//Delete data from prior runs\nval cdbConnector = CassandraConnector(sc)\ncdbConnector.withSessionDo(session => session.execute(\"delete from books_ks.books where book_id in ('b00300','b00001','b00023','b00501','b09999','b01001','b00999','b03999','b02999','b000009');\"))", "outputs": [{"output_type": "stream", "name": "stdout", "text": "res26: com.datastax.driver.core.ResultSet = ResultSet[ exhausted: true, Columns[]]"}], "metadata": {"collapsed": false}}, {"source": "### 3.0.1. Create RDD", "cell_type": "markdown", "metadata": {}}, {"execution_count": 8, "cell_type": "code", "source": "val booksRDD = sc.parallelize(Seq(\n   (\"b00001\", \"Arthur Conan Doyle\", \"A study in scarlet\", 1887),\n   (\"b00023\", \"Arthur Conan Doyle\", \"A sign of four\", 1890),\n   (\"b01001\", \"Arthur Conan Doyle\", \"The adventures of Sherlock Holmes\", 1892),\n   (\"b00501\", \"Arthur Conan Doyle\", \"The memoirs of Sherlock Holmes\", 1893),\n   (\"b00300\", \"Arthur Conan Doyle\", \"The hounds of Baskerville\", 1901)\n))\nbooksRDD.take(2).foreach(println)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "(b00001,Arthur Conan Doyle,A study in scarlet,1887)\n(b00023,Arthur Conan Doyle,A sign of four,1890)"}], "metadata": {"collapsed": false}}, {"source": "### 3.0.2. Persist to Azure Cosmos DB Cassandra API", "cell_type": "markdown", "metadata": {}}, {"execution_count": 9, "cell_type": "code", "source": "import com.datastax.spark.connector.writer._\nbooksRDD.saveToCassandra(\"books_ks\", \"books\", SomeColumns(\"book_id\", \"book_author\", \"book_name\", \"book_pub_year\"),writeConf = WriteConf(ttl = TTLOption.constant(900000),consistencyLevel = ConsistencyLevel.ALL))", "outputs": [], "metadata": {"collapsed": true}}, {"source": "## 4.0. CQL from Spark", "cell_type": "markdown", "metadata": {}}, {"source": "Prefer Dataframe/RDD API for scale", "cell_type": "markdown", "metadata": {}}, {"execution_count": 10, "cell_type": "code", "source": "val cdbConnector = CassandraConnector(sc)\ncdbConnector.withSessionDo(\n  session => session.execute(\"INSERT INTO books_ks.books(book_id, book_name) values('b000009','The Red-Headed League');\"))", "outputs": [{"output_type": "stream", "name": "stdout", "text": "res29: com.datastax.driver.core.ResultSet = ResultSet[ exhausted: true, Columns[]]"}], "metadata": {"collapsed": false}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Spark", "name": "sparkkernel", "language": ""}, "language_info": {"mimetype": "text/x-scala", "pygments_lexer": "scala", "name": "scala", "codemirror_mode": "text/x-scala"}}}