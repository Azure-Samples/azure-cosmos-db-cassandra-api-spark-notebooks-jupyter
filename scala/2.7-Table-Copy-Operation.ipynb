{"nbformat_minor": 2, "cells": [{"source": "# What's in this exercise\nBasics of how to work with Azure Cosmos DB - Cassandra API from Databricks in batch.<br>\n7.0. How to copy data from one table to another<BR>", "cell_type": "markdown", "metadata": {}}, {"source": "### Prerequisites\nThe Datastax connector for Cassandra requires the Azure Comsos DB Cassandra API connection details to be initialized as part of the spark context.  When you launch a Jupyter notebook, the spark session and context are already initialized and it is not advisable to stop and reinitialize the Spark context unless with every configuration set as part of the HDInsight default Jupyter notebook start-up.  One workaround is to add the Cassandra instance details to Ambari, Spark2 service configuration directly.  This is a one-time activity that requires a Spark2 service restart.<BR>\n\n1.  Go to Ambari, Spark2 service and click on configs\n2.  Then go to custom spark2-defaults and add a new property with the following, and restart Spark2 service:\nspark.cassandra.connection.host=YOUR_COSMOSDB_ACCOUNT_NAME.cassandra.cosmosdb.azure.com<br>\nspark.cassandra.connection.port=10350<br>\nspark.cassandra.connection.ssl.enabled=true<br>\nspark.cassandra.auth.username=YOUR_COSMOSDB_ACCOUNT_NAME<br>\nspark.cassandra.auth.password=YOUR_COSMOSDB_KEY<br>", "cell_type": "markdown", "metadata": {}}, {"source": "---------\n## 1.0. Cassandra API connection", "cell_type": "markdown", "metadata": {}}, {"source": "### 1.0.1. Configure dependencies", "cell_type": "markdown", "metadata": {}}, {"execution_count": 1, "cell_type": "code", "source": "%%configure -f\n{ \"conf\": {\"spark.jars.packages\": \"com.datastax.spark:spark-cassandra-connector_2.11:2.3.0,com.microsoft.azure.cosmosdb:azure-cosmos-cassandra-spark-helper:1.0.0\" }}", "outputs": [{"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "Current session configs: <tt>{u'kind': 'spark', u'conf': {u'spark.jars.packages': u'com.datastax.spark:spark-cassandra-connector_2.11:2.3.0,com.microsoft.azure.cosmosdb:azure-cosmos-cassandra-spark-helper:1.0.0'}}</tt><br>"}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "No active sessions."}, "metadata": {}}], "metadata": {"collapsed": false}}, {"source": "### 1.0.2. Cassandra API configuration", "cell_type": "markdown", "metadata": {}}, {"execution_count": 2, "cell_type": "code", "source": "import org.apache.spark.rdd.RDD\nimport org.apache.spark.{SparkConf, SparkContext}\n\nimport spark.implicits._\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.Column\nimport org.apache.spark.sql.types.{StructType, StructField, StringType, IntegerType,LongType,FloatType,DoubleType, TimestampType}\nimport org.apache.spark.sql.cassandra._\n\n//datastax Spark connector\nimport com.datastax.spark.connector._\nimport com.datastax.spark.connector.cql.CassandraConnector\nimport com.datastax.driver.core.{ConsistencyLevel, DataType}\nimport com.datastax.spark.connector.writer.WriteConf\n\n//Azure Cosmos DB library for multiple retry\nimport com.microsoft.azure.cosmosdb.cassandra\n\n// Specify connection factory for Cassandra\nspark.conf.set(\"spark.cassandra.connection.factory\", \"com.microsoft.azure.cosmosdb.cassandra.CosmosDbConnectionFactory\")\n\n// Parallelism and throughput configs\nspark.conf.set(\"spark.cassandra.output.batch.size.rows\", \"1\")\nspark.conf.set(\"spark.cassandra.connection.connections_per_executor_max\", \"10\")\nspark.conf.set(\"spark.cassandra.output.concurrent.writes\", \"100\")\nspark.conf.set(\"spark.cassandra.concurrent.reads\", \"512\")\nspark.conf.set(\"spark.cassandra.output.batch.grouping.buffer.size\", \"1000\")\nspark.conf.set(\"spark.cassandra.connection.keep_alive_ms\", \"60000000\") //Increase this number as needed\nspark.conf.set(\"spark.cassandra.output.ignoreNulls\",\"true\")", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Starting Spark application\n"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>24</td><td>application_1536862968089_0028</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn1-bhoomi.fy0cecrwzcoefky0ef5errkw5g.cx.internal.cloudapp.net:8088/proxy/application_1536862968089_0028/\">Link</a></td><td><a target=\"_blank\" href=\"http://wn0-bhoomi.fy0cecrwzcoefky0ef5errkw5g.cx.internal.cloudapp.net:30060/node/containerlogs/container_e02_1536862968089_0028_01_000001/livy\">Link</a></td><td>\u2714</td></tr></table>"}, "metadata": {}}, {"output_type": "stream", "name": "stdout", "text": "SparkSession available as 'spark'.\n"}], "metadata": {"collapsed": false}}, {"source": "## 2.0. Data generator", "cell_type": "markdown", "metadata": {}}, {"execution_count": 3, "cell_type": "code", "source": "//Delete data from prior runs\nval cdbConnector = CassandraConnector(sc)\ncdbConnector.withSessionDo(session => session.execute(\"delete from books_ks.books where book_id in ('b00300','b00001','b00023','b00501','b09999','b01001','b00999','b03999','b02999','b000009');\"))\n\n//Generate a few rows\nval booksDF = Seq(\n   (\"b00001\", \"Arthur Conan Doyle\", \"A study in scarlet\", 1887,11.33),\n   (\"b00023\", \"Arthur Conan Doyle\", \"A sign of four\", 1890,22.45),\n   (\"b01001\", \"Arthur Conan Doyle\", \"The adventures of Sherlock Holmes\", 1892,19.83),\n   (\"b00501\", \"Arthur Conan Doyle\", \"The memoirs of Sherlock Holmes\", 1893,14.22),\n   (\"b00300\", \"Arthur Conan Doyle\", \"The hounds of Baskerville\", 1901,12.25)\n).toDF(\"book_id\", \"book_author\", \"book_name\", \"book_pub_year\",\"book_price\")\n\n//Persist\nbooksDF.write.mode(\"append\").format(\"org.apache.spark.sql.cassandra\").options(Map( \"table\" -> \"books\", \"keyspace\" -> \"books_ks\", \"output.consistency.level\" -> \"ALL\", \"ttl\" -> \"10000000\")).save()", "outputs": [], "metadata": {"collapsed": true}}, {"source": "---\n## 3.0. Copy from source table to pre-existing destination table", "cell_type": "markdown", "metadata": {}}, {"execution_count": 6, "cell_type": "code", "source": "//1) Create destination table if it does not exist\nval cdbConnector = CassandraConnector(sc)\ncdbConnector.withSessionDo(session => session.execute(\"CREATE TABLE IF NOT EXISTS books_ks.books_copy(book_id TEXT PRIMARY KEY,book_author TEXT, book_name TEXT,book_pub_year INT,book_price FLOAT) WITH cosmosdb_provisioned_throughput=4000;\"))\n\n//2) Delete data from prior runs\ncdbConnector.withSessionDo(session => session.execute(\"DELETE FROM books_ks.books_copy WHERE book_id IN ('b00300','b00001','b00023','b00501','b09999','b01001','b00999','b03999','b02999','b000009');\"))\n\n//3) Read from source table\nval readBooksDF = spark.read.format(\"org.apache.spark.sql.cassandra\").options(Map( \"table\" -> \"books\", \"keyspace\" -> \"books_ks\")).load\n\n//4) Save to destination table\nreadBooksDF.write.cassandraFormat(\"books_copy\", \"books_ks\", \"\").save()\n\n//5) Validate copy to destination table\nspark.read.format(\"org.apache.spark.sql.cassandra\").options(Map( \"table\" -> \"books_copy\", \"keyspace\" -> \"books_ks\")).load.show", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+-------+------------------+--------------------+----------+-------------+\n|book_id|       book_author|           book_name|book_price|book_pub_year|\n+-------+------------------+--------------------+----------+-------------+\n| b00300|Arthur Conan Doyle|The hounds of Bas...|     12.25|         1901|\n| b00001|Arthur Conan Doyle|  A study in scarlet|     11.33|         1887|\n| b00023|Arthur Conan Doyle|      A sign of four|     22.45|         1890|\n| b00501|Arthur Conan Doyle|The memoirs of Sh...|     14.22|         1893|\n| b01001|Arthur Conan Doyle|The adventures of...|     19.83|         1892|\n+-------+------------------+--------------------+----------+-------------+"}], "metadata": {"collapsed": false}}, {"source": "---\n## 4.0. Copy from source table to a non-existent destination table", "cell_type": "markdown", "metadata": {}}, {"execution_count": 14, "cell_type": "code", "source": "import com.datastax.spark.connector._\n\n//1) Clean up from prior executions\nval cdbConnector = CassandraConnector(sc)\ncdbConnector.withSessionDo(session => session.execute(\"DROP TABLE IF EXISTS books_ks.books_new;\"))\n\nThread.sleep(5000) ", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 15, "cell_type": "code", "source": "//2) Read from source table\nval readBooksDF = spark.read.format(\"org.apache.spark.sql.cassandra\").options(Map( \"table\" -> \"books\", \"keyspace\" -> \"books_ks\")).load\n\n//3) Create an empty table in the keyspace based off of source table dataframe\nval newBooksDF = readBooksDF\nnewBooksDF.createCassandraTable(\n    \"books_ks\", \n    \"books_new\", \n    partitionKeyColumns = Some(Seq(\"book_id\"))\n    //clusteringKeyColumns = Some(Seq(\"some column\"))\n    )\n\nThread.sleep(5000) ", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 16, "cell_type": "code", "source": "//4) Save the data from the source table into the newly created table\nnewBooksDF.write.cassandraFormat(\"books_new\", \"books_ks\",\"\").save()", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 17, "cell_type": "code", "source": "//5) Validate table creation and data load\nspark.read.format(\"org.apache.spark.sql.cassandra\").options(Map( \"table\" -> \"books_new\", \"keyspace\" -> \"books_ks\")).load.show", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+-------+------------------+--------------------+----------+-------------+\n|book_id|       book_author|           book_name|book_price|book_pub_year|\n+-------+------------------+--------------------+----------+-------------+\n| b00300|Arthur Conan Doyle|The hounds of Bas...|     12.25|         1901|\n| b00001|Arthur Conan Doyle|  A study in scarlet|     11.33|         1887|\n| b00023|Arthur Conan Doyle|      A sign of four|     22.45|         1890|\n| b00501|Arthur Conan Doyle|The memoirs of Sh...|     14.22|         1893|\n| b01001|Arthur Conan Doyle|The adventures of...|     19.83|         1892|\n+-------+------------------+--------------------+----------+-------------+"}], "metadata": {"collapsed": false}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Spark", "name": "sparkkernel", "language": ""}, "language_info": {"mimetype": "text/x-scala", "pygments_lexer": "scala", "name": "scala", "codemirror_mode": "text/x-scala"}}}