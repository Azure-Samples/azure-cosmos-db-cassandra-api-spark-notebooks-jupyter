{"nbformat_minor": 2, "cells": [{"source": "# What's in this exercise\nBasics of how to work with Azure Cosmos DB-Cassandra API from Databricks <B>in batch</B>.<BR>\nSection 05: Upsert operation (crUd)<BR>", "cell_type": "markdown", "metadata": {}}, {"source": "### Prerequisites\nThe Datastax connector for Cassandra requires the Azure Comsos DB Cassandra API connection details to be initialized as part of the spark context.  When you launch a Jupyter notebook, the spark session and context are already initialized and it is not advisable to stop and reinitialize the Spark context unless with every configuration set as part of the HDInsight default Jupyter notebook start-up.  One workaround is to add the Cassandra instance details to Ambari, Spark2 service configuration directly.  This is a one-time activity that requires a Spark2 service restart.<BR>\n\n1.  Go to Ambari, Spark2 service and click on configs\n2.  Then go to custom spark2-defaults and add a new property with the following, and restart Spark2 service:\nspark.cassandra.connection.host=YOUR_COSMOSDB_ACCOUNT_NAME.cassandra.cosmosdb.azure.com<br>\nspark.cassandra.connection.port=10350<br>\nspark.cassandra.connection.ssl.enabled=true<br>\nspark.cassandra.auth.username=YOUR_COSMOSDB_ACCOUNT_NAME<br>\nspark.cassandra.auth.password=YOUR_COSMOSDB_KEY<br>", "cell_type": "markdown", "metadata": {}}, {"source": "---------\n## 1.0. Cassandra API connection", "cell_type": "markdown", "metadata": {}}, {"source": "### 1.0.1. Configure dependencies", "cell_type": "markdown", "metadata": {}}, {"execution_count": 1, "cell_type": "code", "source": "%%configure -f\n{ \"conf\": {\"spark.jars.packages\": \"com.datastax.spark:spark-cassandra-connector_2.11:2.3.0,com.microsoft.azure.cosmosdb:azure-cosmos-cassandra-spark-helper:1.0.0\" }}", "outputs": [{"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "Current session configs: <tt>{u'kind': 'spark', u'conf': {u'spark.jars.packages': u'com.datastax.spark:spark-cassandra-connector_2.11:2.3.0,com.microsoft.azure.cosmosdb:azure-cosmos-cassandra-spark-helper:1.0.0'}}</tt><br>"}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>13</td><td>application_1536862968089_0017</td><td>spark</td><td>dead</td><td><a target=\"_blank\" href=\"http://hn1-bhoomi.fy0cecrwzcoefky0ef5errkw5g.cx.internal.cloudapp.net:8088/cluster/app/application_1536862968089_0017\">Link</a></td><td></td><td></td></tr><tr><td>17</td><td>application_1536862968089_0021</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn1-bhoomi.fy0cecrwzcoefky0ef5errkw5g.cx.internal.cloudapp.net:8088/proxy/application_1536862968089_0021/\">Link</a></td><td><a target=\"_blank\" href=\"http://wn3-bhoomi.fy0cecrwzcoefky0ef5errkw5g.cx.internal.cloudapp.net:30060/node/containerlogs/container_e02_1536862968089_0021_01_000001/livy\">Link</a></td><td></td></tr></table>"}, "metadata": {}}], "metadata": {"collapsed": false}}, {"source": "### 1.0.2. Cassandra API configuration", "cell_type": "markdown", "metadata": {}}, {"execution_count": 2, "cell_type": "code", "source": "import org.apache.spark.rdd.RDD\nimport org.apache.spark.{SparkConf, SparkContext}\n\nimport spark.implicits._\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.Column\nimport org.apache.spark.sql.types.{StructType, StructField, StringType, IntegerType,LongType,FloatType,DoubleType, TimestampType}\nimport org.apache.spark.sql.cassandra._\n\n//datastax Spark connector\nimport com.datastax.spark.connector._\nimport com.datastax.spark.connector.cql.CassandraConnector\nimport com.datastax.driver.core.{ConsistencyLevel, DataType}\nimport com.datastax.spark.connector.writer.WriteConf\n\n//Azure Cosmos DB library for multiple retry\nimport com.microsoft.azure.cosmosdb.cassandra\n\n// Specify connection factory for Cassandra\nspark.conf.set(\"spark.cassandra.connection.factory\", \"com.microsoft.azure.cosmosdb.cassandra.CosmosDbConnectionFactory\")\n\n// Parallelism and throughput configs\nspark.conf.set(\"spark.cassandra.output.batch.size.rows\", \"1\")\nspark.conf.set(\"spark.cassandra.connection.connections_per_executor_max\", \"10\")\nspark.conf.set(\"spark.cassandra.output.concurrent.writes\", \"100\")\nspark.conf.set(\"spark.cassandra.concurrent.reads\", \"512\")\nspark.conf.set(\"spark.cassandra.output.batch.grouping.buffer.size\", \"1000\")\nspark.conf.set(\"spark.cassandra.connection.keep_alive_ms\", \"60000000\") //Increase this number as needed\nspark.conf.set(\"spark.cassandra.output.ignoreNulls\",\"true\")", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Starting Spark application\n"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>20</td><td>application_1536862968089_0024</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn1-bhoomi.fy0cecrwzcoefky0ef5errkw5g.cx.internal.cloudapp.net:8088/proxy/application_1536862968089_0024/\">Link</a></td><td><a target=\"_blank\" href=\"http://wn0-bhoomi.fy0cecrwzcoefky0ef5errkw5g.cx.internal.cloudapp.net:30060/node/containerlogs/container_e02_1536862968089_0024_01_000001/livy\">Link</a></td><td>\u2714</td></tr></table>"}, "metadata": {}}, {"output_type": "stream", "name": "stdout", "text": "SparkSession available as 'spark'.\n"}], "metadata": {"collapsed": false}}, {"source": "---\n## 2.0. Dataframe API", "cell_type": "markdown", "metadata": {}}, {"source": "### Reset data", "cell_type": "markdown", "metadata": {}}, {"execution_count": 10, "cell_type": "code", "source": "//Delete data from prior runs\nval cdbConnector = CassandraConnector(sc)\ncdbConnector.withSessionDo(session => session.execute(\"delete from books_ks.books where book_id in ('b00300','b00001','b00023','b00501','b09999','b01001','b00999','b03999','b02999','b000009');\"))\n\n//Create 5 records and persist\nval booksDF = Seq(\n   (\"b00001\", \"Arthur Conan Doyle\", \"A study in scarlet\", 1887),\n   (\"b00023\", \"Arthur Conan Doyle\", \"A sign of four\", 1890),\n   (\"b01001\", \"Arthur Conan Doyle\", \"The adventures of Sherlock Holmes\", 1892),\n   (\"b00501\", \"Arthur Conan Doyle\", \"The memoirs of Sherlock Holmes\", 1893),\n   (\"b00300\", \"Arthur Conan Doyle\", \"The hounds of Baskerville\", 1901)\n).toDF(\"book_id\", \"book_author\", \"book_name\", \"book_pub_year\")\n\nbooksDF.write.mode(\"append\").format(\"org.apache.spark.sql.cassandra\").options(Map( \"table\" -> \"books\", \"keyspace\" -> \"books_ks\", \"output.consistency.level\" -> \"ALL\", \"ttl\" -> \"10000000\")).save()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "res36: com.datastax.driver.core.ResultSet = ResultSet[ exhausted: true, Columns[]]"}], "metadata": {"collapsed": false}}, {"source": "### 2.0.1. Upsert", "cell_type": "markdown", "metadata": {}}, {"execution_count": 12, "cell_type": "code", "source": "//Before\nspark.read.cassandraFormat(\"books\", \"books_ks\", \"\").load().show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+-------+------------------+--------------------+----------+-------------+\n|book_id|       book_author|           book_name|book_price|book_pub_year|\n+-------+------------------+--------------------+----------+-------------+\n| b00300|Arthur Conan Doyle|The hounds of Bas...|      null|         1901|\n| b00001|Arthur Conan Doyle|  A study in scarlet|      null|         1887|\n| b00023|Arthur Conan Doyle|      A sign of four|      null|         1890|\n| b00501|Arthur Conan Doyle|The memoirs of Sh...|      null|         1893|\n| b01001|Arthur Conan Doyle|The adventures of...|      null|         1892|\n+-------+------------------+--------------------+----------+-------------+"}], "metadata": {"collapsed": false}}, {"execution_count": 13, "cell_type": "code", "source": "// Create a dataframe with changes you want to make\n//(1) Update: Changing author name to include prefix of \"Sir\", and (2) Insert: adding a new book\nval booksUpsertDF = Seq(\n                         (\"b00001\", \"Sir Arthur Conan Doyle\", \"A study in scarlet\", 1887),\n                         (\"b00023\", \"Sir Arthur Conan Doyle\", \"A sign of four\", 1890),\n                         (\"b01001\", \"Sir Arthur Conan Doyle\", \"The adventures of Sherlock Holmes\", 1892),\n                         (\"b00501\", \"Sir Arthur Conan Doyle\", \"The memoirs of Sherlock Holmes\", 1893),\n                         (\"b00300\", \"Sir Arthur Conan Doyle\", \"The hounds of Baskerville\", 1901),\n                         (\"b09999\", \"Sir Arthur Conan Doyle\", \"The return of Sherlock Holmes\", 1905)\n                        ).toDF(\"book_id\", \"book_author\", \"book_name\", \"book_pub_year\")", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+-------+--------------------+--------------------+-------------+\n|book_id|         book_author|           book_name|book_pub_year|\n+-------+--------------------+--------------------+-------------+\n| b00001|Sir Arthur Conan ...|  A study in scarlet|         1887|\n| b00023|Sir Arthur Conan ...|      A sign of four|         1890|\n| b01001|Sir Arthur Conan ...|The adventures of...|         1892|\n| b00501|Sir Arthur Conan ...|The memoirs of Sh...|         1893|\n| b00300|Sir Arthur Conan ...|The hounds of Bas...|         1901|\n| b09999|Sir Arthur Conan ...|The return of She...|         1905|\n+-------+--------------------+--------------------+-------------+"}], "metadata": {"collapsed": false}}, {"execution_count": 14, "cell_type": "code", "source": "// Upsert\nbooksUpsertDF.write.mode(\"append\").format(\"org.apache.spark.sql.cassandra\").options(Map( \"table\" -> \"books\", \"keyspace\" -> \"books_ks\")).save()", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 15, "cell_type": "code", "source": "//After\nspark.read.cassandraFormat(\"books\", \"books_ks\", \"\").load().show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+-------+--------------------+--------------------+----------+-------------+\n|book_id|         book_author|           book_name|book_price|book_pub_year|\n+-------+--------------------+--------------------+----------+-------------+\n| b00300|Sir Arthur Conan ...|The hounds of Bas...|      null|         1901|\n| b00001|Sir Arthur Conan ...|  A study in scarlet|      null|         1887|\n| b00023|Sir Arthur Conan ...|      A sign of four|      null|         1890|\n| b00501|Sir Arthur Conan ...|The memoirs of Sh...|      null|         1893|\n| b09999|Sir Arthur Conan ...|The return of She...|      null|         1905|\n| b01001|Sir Arthur Conan ...|The adventures of...|      null|         1892|\n+-------+--------------------+--------------------+----------+-------------+"}], "metadata": {"collapsed": false}}, {"source": "### 2.0.2. Update", "cell_type": "markdown", "metadata": {}}, {"execution_count": 16, "cell_type": "code", "source": "//Before\nspark.read.cassandraFormat(\"books\", \"books_ks\", \"\").load().show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+-------+--------------------+--------------------+----------+-------------+\n|book_id|         book_author|           book_name|book_price|book_pub_year|\n+-------+--------------------+--------------------+----------+-------------+\n| b00300|Sir Arthur Conan ...|The hounds of Bas...|      null|         1901|\n| b00001|Sir Arthur Conan ...|  A study in scarlet|      null|         1887|\n| b00023|Sir Arthur Conan ...|      A sign of four|      null|         1890|\n| b00501|Sir Arthur Conan ...|The memoirs of Sh...|      null|         1893|\n| b09999|Sir Arthur Conan ...|The return of She...|      null|         1905|\n| b01001|Sir Arthur Conan ...|The adventures of...|      null|         1892|\n+-------+--------------------+--------------------+----------+-------------+"}], "metadata": {"collapsed": false}}, {"execution_count": 17, "cell_type": "code", "source": "//Update\nval booksUpdateDF = Seq(\n                         (\"b00001\", 5.99),\n                         (\"b00023\", 7.50),\n                         (\"b01001\", 12.25),\n                         (\"b00501\", 12.00),\n                         (\"b00300\", 18.00),\n                         (\"b09999\", 23.99)\n                        ).toDF(\"book_id\", \"book_price\")\nbooksUpdateDF.write.mode(\"append\").format(\"org.apache.spark.sql.cassandra\").options(Map( \"table\" -> \"books\", \"keyspace\" -> \"books_ks\")).save()", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 18, "cell_type": "code", "source": "//After\nspark.read.cassandraFormat(\"books\", \"books_ks\", \"\").load().show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+-------+--------------------+--------------------+----------+-------------+\n|book_id|         book_author|           book_name|book_price|book_pub_year|\n+-------+--------------------+--------------------+----------+-------------+\n| b00300|Sir Arthur Conan ...|The hounds of Bas...|      18.0|         1901|\n| b00001|Sir Arthur Conan ...|  A study in scarlet|      5.99|         1887|\n| b00023|Sir Arthur Conan ...|      A sign of four|       7.5|         1890|\n| b00501|Sir Arthur Conan ...|The memoirs of Sh...|      12.0|         1893|\n| b09999|Sir Arthur Conan ...|The return of She...|     23.99|         1905|\n| b01001|Sir Arthur Conan ...|The adventures of...|     12.25|         1892|\n+-------+--------------------+--------------------+----------+-------------+"}], "metadata": {"collapsed": false}}, {"source": "---\n## 3.0. RDD API\nUpserts, no different inserts/creates", "cell_type": "markdown", "metadata": {}}, {"source": "---\n## 4.0. Upserts with CQL", "cell_type": "markdown", "metadata": {}}, {"execution_count": 20, "cell_type": "code", "source": "//Runs on driver, use wisely\ncdbConnector.withSessionDo(session => session.execute(\"update books_ks.books set book_price=99.33 where book_id ='b00300';\"))", "outputs": [{"output_type": "stream", "name": "stdout", "text": "res58: com.datastax.driver.core.ResultSet = ResultSet[ exhausted: true, Columns[]]"}], "metadata": {"collapsed": false}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Spark", "name": "sparkkernel", "language": ""}, "language_info": {"mimetype": "text/x-scala", "pygments_lexer": "scala", "name": "scala", "codemirror_mode": "text/x-scala"}}}