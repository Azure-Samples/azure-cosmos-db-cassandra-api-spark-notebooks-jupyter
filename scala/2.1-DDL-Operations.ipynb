{"nbformat_minor": 2, "cells": [{"source": "# What's in this exercise\nBasics of how to work with Azure Cosmos DB Cassandra API from Zeppelin <B>in batch</B>.<br>\nSection 01: Cassandra API connection<br>\nSection 02: Keyspace DDL<br>\nSection 03: Table DDL<br>\n  \n**Reference:** \nhttps://github.com/datastax/spark-cassandra-connector/blob/master/doc/1_connecting.md", "cell_type": "markdown", "metadata": {}}, {"source": "### Prerequisites\nThe Datastax connector for Cassandra requires the Azure Comsos DB Cassandra API connection details to be initialized as part of the spark context.  When you launch a Jupyter notebook, the spark session and context are already initialized and it is not advisable to stop and reinitialize the Spark context unless with every configuration set as part of the HDInsight default Jupyter notebook start-up.  One workaround is to add the Cassandra instance details to Ambari, Spark2 service configuration directly.  This is a one-time activity that requires a Spark2 service restart.<BR>\n\n1.  Go to Ambari, Spark2 service and click on configs\n2.  Then go to custom spark2-defaults and add a new property with the following, and restart Spark2 service:\nspark.cassandra.connection.host=YOUR_COSMOSDB_ACCOUNT_NAME.cassandra.cosmosdb.azure.com<br>\nspark.cassandra.connection.port=10350<br>\nspark.cassandra.connection.ssl.enabled=true<br>\nspark.cassandra.auth.username=YOUR_COSMOSDB_ACCOUNT_NAME<br>\nspark.cassandra.auth.password=YOUR_COSMOSDB_KEY<br>\n", "cell_type": "markdown", "metadata": {}}, {"source": "---------\n## 1.0. Cassandra API connection", "cell_type": "markdown", "metadata": {}}, {"source": "### 1.0.1. Configure dependencies", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "%%configure -f\n{ \"conf\": {\"spark.jars.packages\": \"com.datastax.spark:spark-cassandra-connector_2.11:2.3.0,com.microsoft.azure.cosmosdb:azure-cosmos-cassandra-spark-helper:1.0.0\" }}", "outputs": [], "metadata": {"collapsed": false}}, {"source": "### 1.0.2. Cassandra API configuration", "cell_type": "markdown", "metadata": {}}, {"execution_count": 2, "cell_type": "code", "source": "import org.apache.spark.rdd.RDD\nimport org.apache.spark.{SparkConf, SparkContext}\n\nimport spark.implicits._\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.Column\n\nimport org.apache.spark.sql.cassandra._\n\n//datastax Spark connector\nimport com.datastax.spark.connector._\nimport com.datastax.spark.connector.cql.CassandraConnector\n\n//CosmosDB library for multiple retry\nimport com.microsoft.azure.cosmosdb.cassandra\n\n// Specify connection factory for Cassandra\nspark.conf.set(\"spark.cassandra.connection.factory\", \"com.microsoft.azure.cosmosdb.cassandra.CosmosDbConnectionFactory\")\n\n// Parallelism and throughput configs - increase as needed to tune your jobs\nspark.conf.set(\"spark.cassandra.output.batch.size.rows\", \"1\")//Do not modify this\nspark.conf.set(\"spark.cassandra.connection.connections_per_executor_max\", \"10\")\nspark.conf.set(\"spark.cassandra.output.concurrent.writes\", \"100\")\nspark.conf.set(\"spark.cassandra.concurrent.reads\", \"512\")\nspark.conf.set(\"spark.cassandra.output.batch.grouping.buffer.size\", \"1000\")\nspark.conf.set(\"spark.cassandra.connection.keep_alive_ms\", \"60000000\") \nspark.conf.set(\"spark.cassandra.output.ignoreNulls\",\"true\")\n\n", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Starting Spark application\n"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>13</td><td>application_1536862968089_0017</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn1-bhoomi.fy0cecrwzcoefky0ef5errkw5g.cx.internal.cloudapp.net:8088/proxy/application_1536862968089_0017/\">Link</a></td><td><a target=\"_blank\" href=\"http://wn0-bhoomi.fy0cecrwzcoefky0ef5errkw5g.cx.internal.cloudapp.net:30060/node/containerlogs/container_e02_1536862968089_0017_01_000001/livy\">Link</a></td><td>\u2714</td></tr></table>"}, "metadata": {}}, {"output_type": "stream", "name": "stdout", "text": "SparkSession available as 'spark'.\n"}], "metadata": {"collapsed": false}}, {"source": "-----\n## 2.0. Cassandra Keyspace DDL operations", "cell_type": "markdown", "metadata": {}}, {"source": "### 2.0.a. Create keyspace - from Spark", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "val cdbConnector = CassandraConnector(sc)\n// Create keyspace\ncdbConnector.withSessionDo(session => session.execute(\"CREATE KEYSPACE IF NOT EXISTS books_ks WITH REPLICATION = {'class': 'SimpleStrategy', 'replication_factor': 1 } \"))", "outputs": [], "metadata": {"collapsed": false}}, {"source": "### 2.0.b. Alter keyspace - from Spark\nNot supported currently.", "cell_type": "markdown", "metadata": {}}, {"source": "### 2.0.c. Delete keyspace - from Spark", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "val cdbConnector = CassandraConnector(sc)\n//cdbConnector.withSessionDo(session => session.execute(\"DROP KEYSPACE books_ks\"))", "outputs": [], "metadata": {"collapsed": false}}, {"source": "-----\n## 3.0. Cassandra Table DDL operations", "cell_type": "markdown", "metadata": {}}, {"source": "**Considerations:**<br>\n&nbsp;&nbsp;-Throughput can be provisioned at a table level as part of the create table statement.<br>\n&nbsp;&nbsp;-One partition key can store 10 GB of data. <br> \n&nbsp;&nbsp;-One record can be max of 2 MB in size<br>\n&nbsp;&nbsp;-One partition key range can store multiple partition keys.<br>", "cell_type": "markdown", "metadata": {}}, {"source": "### 3.0.a. Create table - from cqlsh", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "val cdbConnector = CassandraConnector(sc)\ncdbConnector.withSessionDo(session => session.execute(\"CREATE TABLE IF NOT EXISTS books_ks.books(book_id TEXT PRIMARY KEY,book_author TEXT, book_name TEXT,book_pub_year INT,book_price FLOAT) WITH cosmosdb_provisioned_throughput=4000 , WITH default_time_to_live=630720000;\"))", "outputs": [], "metadata": {"collapsed": false}}, {"source": "### 3.0.b. Alter table \n**Considerations:**<BR>\n(1) Alter table - add/change columns - on the roadmap<BR>\n(2) Alter provisioned throughput from Spark - on the roadmap <BR>\n(3) Alter table TTL - on the roadmap <BR>", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "//val cdbConnector = CassandraConnector(sc)\n//cdbConnector.withSessionDo(session => session.execute(\"ALTER TABLE books_ks.books WITH cosmosdb_provisioned_throughput=8000, WITH default_time_to_live=0;\"))", "outputs": [], "metadata": {"collapsed": false}}, {"source": "### 3.0.c. Drop table ", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "val cdbConnector = CassandraConnector(sc)\n//cdbConnector.withSessionDo(session => session.execute(\"DROP TABLE IF EXISTS books_ks.books;\"))", "outputs": [], "metadata": {"collapsed": false}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Spark", "name": "sparkkernel", "language": ""}, "language_info": {"mimetype": "text/x-scala", "pygments_lexer": "scala", "name": "scala", "codemirror_mode": "text/x-scala"}}}