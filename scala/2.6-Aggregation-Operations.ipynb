{"nbformat_minor": 2, "cells": [{"source": "# What's in this exercise\nBasics of how to work with Azure Cosmos DB - Cassandra API from Databricks <B>in batch</B>.<BR>\nSection 07: Aggregation operations<BR>\n  \n**NOTE:**<br>\n1) Server-side(Cassandra) filtering of non-partition key columns is not yet supported.<BR>\n2) Server-side(Cassandra) aggregation operations are not yet supported yet<BR>\nThe samples below perform the same on the Spark-side<br>", "cell_type": "markdown", "metadata": {}}, {"source": "### Prerequisites\nThe Datastax connector for Cassandra requires the Azure Comsos DB Cassandra API connection details to be initialized as part of the spark context.  When you launch a Jupyter notebook, the spark session and context are already initialized and it is not advisable to stop and reinitialize the Spark context unless with every configuration set as part of the HDInsight default Jupyter notebook start-up.  One workaround is to add the Cassandra instance details to Ambari, Spark2 service configuration directly.  This is a one-time activity that requires a Spark2 service restart.<BR>\n\n1.  Go to Ambari, Spark2 service and click on configs\n2.  Then go to custom spark2-defaults and add a new property with the following, and restart Spark2 service:\nspark.cassandra.connection.host=YOUR_COSMOSDB_ACCOUNT_NAME.cassandra.cosmosdb.azure.com<br>\nspark.cassandra.connection.port=10350<br>\nspark.cassandra.connection.ssl.enabled=true<br>\nspark.cassandra.auth.username=YOUR_COSMOSDB_ACCOUNT_NAME<br>\nspark.cassandra.auth.password=YOUR_COSMOSDB_KEY<br>", "cell_type": "markdown", "metadata": {}}, {"source": "---------\n## 1.0. Cassandra API connection", "cell_type": "markdown", "metadata": {}}, {"source": "### 1.0.1. Configure dependencies", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "%%configure -f\n{ \"conf\": {\"spark.jars.packages\": \"com.datastax.spark:spark-cassandra-connector_2.11:2.3.0,com.microsoft.azure.cosmosdb:azure-cosmos-cassandra-spark-helper:1.0.0\" }}", "outputs": [], "metadata": {"collapsed": false}}, {"source": "### 1.0.2. Cassandra API configuration", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "import org.apache.spark.rdd.RDD\nimport org.apache.spark.{SparkConf, SparkContext}\n\nimport spark.implicits._\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.Column\nimport org.apache.spark.sql.types.{StructType, StructField, StringType, IntegerType,LongType,FloatType,DoubleType, TimestampType}\nimport org.apache.spark.sql.cassandra._\n\n//datastax Spark connector\nimport com.datastax.spark.connector._\nimport com.datastax.spark.connector.cql.CassandraConnector\nimport com.datastax.driver.core.{ConsistencyLevel, DataType}\nimport com.datastax.spark.connector.writer.WriteConf\n\n//Azure Cosmos DB library for multiple retry\nimport com.microsoft.azure.cosmosdb.cassandra\n\n// Specify connection factory for Cassandra\nspark.conf.set(\"spark.cassandra.connection.factory\", \"com.microsoft.azure.cosmosdb.cassandra.CosmosDbConnectionFactory\")\n\n// Parallelism and throughput configs\nspark.conf.set(\"spark.cassandra.output.batch.size.rows\", \"1\")\nspark.conf.set(\"spark.cassandra.connection.connections_per_executor_max\", \"10\")\nspark.conf.set(\"spark.cassandra.output.concurrent.writes\", \"100\")\nspark.conf.set(\"spark.cassandra.concurrent.reads\", \"512\")\nspark.conf.set(\"spark.cassandra.output.batch.grouping.buffer.size\", \"1000\")\nspark.conf.set(\"spark.cassandra.connection.keep_alive_ms\", \"60000000\") //Increase this number as needed\nspark.conf.set(\"spark.cassandra.output.ignoreNulls\",\"true\")", "outputs": [], "metadata": {"collapsed": false}}, {"source": "## 2.0. Data generator", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "//Delete data from prior runs\nval cdbConnector = CassandraConnector(sc)\ncdbConnector.withSessionDo(session => session.execute(\"delete from books_ks.books where book_id in ('b00300','b00001','b00023','b00501','b09999','b01001','b00999','b03999','b02999','b000009');\"))\n\n//Generate a few rows\nval booksDF = Seq(\n   (\"b00001\", \"Arthur Conan Doyle\", \"A study in scarlet\", 1887,11.33),\n   (\"b00023\", \"Arthur Conan Doyle\", \"A sign of four\", 1890,22.45),\n   (\"b01001\", \"Arthur Conan Doyle\", \"The adventures of Sherlock Holmes\", 1892,19.83),\n   (\"b00501\", \"Arthur Conan Doyle\", \"The memoirs of Sherlock Holmes\", 1893,14.22),\n   (\"b00300\", \"Arthur Conan Doyle\", \"The hounds of Baskerville\", 1901,12.25)\n).toDF(\"book_id\", \"book_author\", \"book_name\", \"book_pub_year\",\"book_price\")\n\n//Persist\nbooksDF.write.mode(\"append\").format(\"org.apache.spark.sql.cassandra\").options(Map( \"table\" -> \"books\", \"keyspace\" -> \"books_ks\", \"output.consistency.level\" -> \"ALL\", \"ttl\" -> \"10000000\")).save()", "outputs": [], "metadata": {"collapsed": true}}, {"source": "---\n## 3.0. Count", "cell_type": "markdown", "metadata": {}}, {"source": "### 3.0.1. RDD API", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "sc.cassandraTable(\"books_ks\", \"books\").count", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "//count on server side - NOT SUPPORTED YET\n//sc.cassandraTable(\"books_ks\", \"books\").cassandraCount", "outputs": [], "metadata": {"collapsed": true}}, {"source": "### 3.0.2. Dataframe API", "cell_type": "markdown", "metadata": {"collapsed": false}}, {"source": "Count does not work currently for the dataframe API.<BR>\nWhile we are pending release of count support, the sample below shows how we can execute counts currently using dataframe caching as a workaround-<br>\n  \n**Options for storage level**<br>\nhttps://spark.apache.org/docs/2.2.0/rdd-programming-guide.html#which-storage-level-to-choose<br>\n(1) MEMORY_ONLY:\t\nStore RDD as deserialized Java objects in the JVM. If the RDD does not fit in memory, some partitions will not be cached and will be recomputed on the fly each time they're needed. This is the default level.<br>\n(2) MEMORY_AND_DISK:\t<br>\nStore RDD as deserialized Java objects in the JVM. If the RDD does not fit in memory, store the partitions that don't fit on disk, and read them from there when they're needed.<br>\n(3) MEMORY_ONLY_SER: Java/Scala<br>\nStore RDD as serialized Java objects (one byte array per partition). This is generally more space-efficient than deserialized objects, especially when using a fast serializer, but more CPU-intensive to read.<br>\n(4) MEMORY_AND_DISK_SER:  Java/Scala<br>\nSimilar to MEMORY_ONLY_SER, but spill partitions that don't fit in memory to disk instead of recomputing them on the fly each time they're needed.<br>\n(5) DISK_ONLY:\t<br>\nStore the RDD partitions only on disk.<br>\n(6) MEMORY_ONLY_2, MEMORY_AND_DISK_2, etc.\t<br>\nSame as the levels above, but replicate each partition on two cluster nodes.<br>\n(7) OFF_HEAP (experimental):<br>\nSimilar to MEMORY_ONLY_SER, but store the data in off-heap memory. This requires off-heap memory to be enabled.<br>", "cell_type": "markdown", "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "//Workaround\nimport org.apache.spark.storage.StorageLevel\n\n//Read from source\nval readBooksDF = spark.read.cassandraFormat(\"books\", \"books_ks\", \"\").load()\n\n//Explain plan\nreadBooksDF.explain", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "//Materialize the dataframe\nreadBooksDF.persist(StorageLevel.MEMORY_ONLY)\n\n//Subsequent execution against this DF hits the cache \nreadBooksDF.count", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "//Persist as temporary view\nreadBooksDF.createOrReplaceTempView(\"books_vw\")", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "%%sql\n--select * from books_vw\n--select count(*) from books_vw where book_pub_year > 1900\n--select count(book_id) from books_vw\nselect book_author, count(*) as count from books_vw group by book_author\n", "outputs": [], "metadata": {"collapsed": false}}, {"source": "## 4.0. Average", "cell_type": "markdown", "metadata": {}}, {"source": "### 4.0.1. RDD API", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "sc.cassandraTable(\"books_ks\", \"books\").select(\"book_price\").as((c: Double) => c).mean", "outputs": [], "metadata": {"collapsed": false}}, {"source": "### 4.0.2. Dataframe API", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "spark.read.cassandraFormat(\"books\", \"books_ks\", \"\").load().select(\"book_price\").agg(avg(\"book_price\")).show", "outputs": [], "metadata": {"collapsed": false}}, {"source": "### 4.0.3. SQL ", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "%%sql\nselect min(book_price) from books_vw", "outputs": [], "metadata": {"collapsed": false}}, {"source": "## 5.0. Min", "cell_type": "markdown", "metadata": {}}, {"source": "### 5.0.1. RDD API", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "sc.cassandraTable(\"books_ks\", \"books\").select(\"book_price\").as((c: Float) => c).min", "outputs": [], "metadata": {"collapsed": false}}, {"source": "### 5.0.2. Dataframe API", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "spark.read.cassandraFormat(\"books\", \"books_ks\", \"\").load().select(\"book_id\",\"book_price\").agg(min(\"book_price\")).show", "outputs": [], "metadata": {"collapsed": false}}, {"source": "### 5.0.3. SQL", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "%%sql\nselect min(book_price) from books_vw", "outputs": [], "metadata": {"collapsed": false}}, {"source": "## 6.0. Max", "cell_type": "markdown", "metadata": {}}, {"source": "### 6.0.1. RDD API", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "sc.cassandraTable(\"books_ks\", \"books\").select(\"book_price\").as((c: Float) => c).max", "outputs": [], "metadata": {"collapsed": false}}, {"source": "### 6.0.2. Dataframe API", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "spark.read.cassandraFormat(\"books\", \"books_ks\", \"\").load().select(\"book_price\").agg(max(\"book_price\")).show", "outputs": [], "metadata": {"collapsed": false}}, {"source": "### 6.0.3. SQL", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "%%sql\nselect max(book_price) from books_vw", "outputs": [], "metadata": {"collapsed": false}}, {"source": "## 7.0. Sum", "cell_type": "markdown", "metadata": {}}, {"source": "### 7.0.1. RDD API", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "sc.cassandraTable(\"books_ks\", \"books\").select(\"book_price\").as((c: Float) => c).sum", "outputs": [], "metadata": {"collapsed": false}}, {"source": "### 7.0.2. Dataframe API", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "spark.read.cassandraFormat(\"books\", \"books_ks\", \"\").load().select(\"book_price\").agg(sum(\"book_price\")).show", "outputs": [], "metadata": {"collapsed": false}}, {"source": "### 7.0.3. SQL", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "%%sql\nselect sum(book_price) from books_vw", "outputs": [], "metadata": {"collapsed": false}}, {"source": "## 8.0. Top or comparable", "cell_type": "markdown", "metadata": {}}, {"source": "### 8.0.1. RDD API", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "val readCalcTopRDD = sc.cassandraTable(\"books_ks\", \"books\").select(\"book_name\",\"book_price\").sortBy(_.getFloat(1), false)\nreadCalcTopRDD.zipWithIndex.filter(_._2 < 3).collect.foreach(println)\n//delivers the first top n items without collecting the rdd to the driver.", "outputs": [], "metadata": {"collapsed": false}}, {"source": "### 8.0.2. Dataframe API", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "import org.apache.spark.sql.functions._\n\nval readBooksDF = spark.read.format(\"org.apache.spark.sql.cassandra\").options(Map( \"table\" -> \"books\", \"keyspace\" -> \"books_ks\")).load.select(\"book_name\",\"book_price\").orderBy(desc(\"book_price\")).limit(3)\n\n//Explain plan\nreadBooksDF.explain", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "//Top 3\nreadBooksDF.show", "outputs": [], "metadata": {"collapsed": false}}, {"source": "### 8.0.3. SQL", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "%%sql\nselect book_name,book_price from books_vw order by book_price desc limit 3", "outputs": [], "metadata": {"collapsed": false}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Spark", "name": "sparkkernel", "language": ""}, "language_info": {"mimetype": "text/x-scala", "pygments_lexer": "scala", "name": "scala", "codemirror_mode": "text/x-scala"}}}