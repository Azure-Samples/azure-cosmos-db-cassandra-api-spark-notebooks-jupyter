{"nbformat_minor": 2, "cells": [{"source": "# What's in this exercise\nBasics of how to work with Azure Cosmos DB-Cassandra API from Databricks <B>in batch</B>.<BR>\nSection 06: Delete operation (crUd)<BR>", "cell_type": "markdown", "metadata": {}}, {"source": "### Prerequisites\nThe Datastax connector for Cassandra requires the Azure Comsos DB Cassandra API connection details to be initialized as part of the spark context.  When you launch a Jupyter notebook, the spark session and context are already initialized and it is not advisable to stop and reinitialize the Spark context unless with every configuration set as part of the HDInsight default Jupyter notebook start-up.  One workaround is to add the Cassandra instance details to Ambari, Spark2 service configuration directly.  This is a one-time activity that requires a Spark2 service restart.<BR>\n\n1.  Go to Ambari, Spark2 service and click on configs\n2.  Then go to custom spark2-defaults and add a new property with the following, and restart Spark2 service:\nspark.cassandra.connection.host=YOUR_COSMOSDB_ACCOUNT_NAME.cassandra.cosmosdb.azure.com<br>\nspark.cassandra.connection.port=10350<br>\nspark.cassandra.connection.ssl.enabled=true<br>\nspark.cassandra.auth.username=YOUR_COSMOSDB_ACCOUNT_NAME<br>\nspark.cassandra.auth.password=YOUR_COSMOSDB_KEY<br>", "cell_type": "markdown", "metadata": {}}, {"source": "---------\n## 1.0. Cassandra API connection", "cell_type": "markdown", "metadata": {}}, {"source": "### 1.0.1. Configure dependencies", "cell_type": "markdown", "metadata": {}}, {"execution_count": 1, "cell_type": "code", "source": "%%configure -f\n{ \"conf\": {\"spark.jars.packages\": \"com.datastax.spark:spark-cassandra-connector_2.11:2.3.0,com.microsoft.azure.cosmosdb:azure-cosmos-cassandra-spark-helper:1.0.0\" }}", "outputs": [{"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "Current session configs: <tt>{u'kind': 'spark', u'conf': {u'spark.jars.packages': u'com.datastax.spark:spark-cassandra-connector_2.11:2.3.0,com.microsoft.azure.cosmosdb:azure-cosmos-cassandra-spark-helper:1.0.0'}}</tt><br>"}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>13</td><td>application_1536862968089_0017</td><td>spark</td><td>dead</td><td><a target=\"_blank\" href=\"http://hn1-bhoomi.fy0cecrwzcoefky0ef5errkw5g.cx.internal.cloudapp.net:8088/cluster/app/application_1536862968089_0017\">Link</a></td><td></td><td></td></tr><tr><td>17</td><td>application_1536862968089_0021</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn1-bhoomi.fy0cecrwzcoefky0ef5errkw5g.cx.internal.cloudapp.net:8088/proxy/application_1536862968089_0021/\">Link</a></td><td><a target=\"_blank\" href=\"http://wn3-bhoomi.fy0cecrwzcoefky0ef5errkw5g.cx.internal.cloudapp.net:30060/node/containerlogs/container_e02_1536862968089_0021_01_000001/livy\">Link</a></td><td></td></tr></table>"}, "metadata": {}}], "metadata": {"collapsed": false}}, {"source": "### 1.0.2. Cassandra API configuration", "cell_type": "markdown", "metadata": {}}, {"execution_count": 2, "cell_type": "code", "source": "import org.apache.spark.rdd.RDD\nimport org.apache.spark.{SparkConf, SparkContext}\n\nimport spark.implicits._\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.Column\nimport org.apache.spark.sql.types.{StructType, StructField, StringType, IntegerType,LongType,FloatType,DoubleType, TimestampType}\nimport org.apache.spark.sql.cassandra._\n\n//datastax Spark connector\nimport com.datastax.spark.connector._\nimport com.datastax.spark.connector.cql.CassandraConnector\nimport com.datastax.driver.core.{ConsistencyLevel, DataType}\nimport com.datastax.spark.connector.writer.WriteConf\n\n//Azure Cosmos DB library for multiple retry\nimport com.microsoft.azure.cosmosdb.cassandra\n\n// Specify connection factory for Cassandra\nspark.conf.set(\"spark.cassandra.connection.factory\", \"com.microsoft.azure.cosmosdb.cassandra.CosmosDbConnectionFactory\")\n\n// Parallelism and throughput configs\nspark.conf.set(\"spark.cassandra.output.batch.size.rows\", \"1\")\nspark.conf.set(\"spark.cassandra.connection.connections_per_executor_max\", \"10\")\nspark.conf.set(\"spark.cassandra.output.concurrent.writes\", \"100\")\nspark.conf.set(\"spark.cassandra.concurrent.reads\", \"512\")\nspark.conf.set(\"spark.cassandra.output.batch.grouping.buffer.size\", \"1000\")\nspark.conf.set(\"spark.cassandra.connection.keep_alive_ms\", \"60000000\") //Increase this number as needed\nspark.conf.set(\"spark.cassandra.output.ignoreNulls\",\"true\")", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Starting Spark application\n"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>21</td><td>application_1536862968089_0025</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn1-bhoomi.fy0cecrwzcoefky0ef5errkw5g.cx.internal.cloudapp.net:8088/proxy/application_1536862968089_0025/\">Link</a></td><td><a target=\"_blank\" href=\"http://wn2-bhoomi.fy0cecrwzcoefky0ef5errkw5g.cx.internal.cloudapp.net:30060/node/containerlogs/container_e02_1536862968089_0025_01_000001/livy\">Link</a></td><td>\u2714</td></tr></table>"}, "metadata": {}}, {"output_type": "stream", "name": "stdout", "text": "SparkSession available as 'spark'.\n"}], "metadata": {"collapsed": false}}, {"source": "## 2.0. Data generator", "cell_type": "markdown", "metadata": {}}, {"execution_count": 23, "cell_type": "code", "source": "//Delete data from prior runs\nval cdbConnector = CassandraConnector(sc)\ncdbConnector.withSessionDo(session => session.execute(\"delete from books_ks.books where book_id in ('b00300','b00001','b00023','b00501','b09999','b01001','b00999','b03999','b02999','b000009');\"))\n\n//Generate a few rows\nval booksDF = Seq(\n   (\"b00001\", \"Arthur Conan Doyle\", \"A study in scarlet\", 1887,11.33),\n   (\"b00023\", \"Arthur Conan Doyle\", \"A sign of four\", 1890,22.45),\n   (\"b01001\", \"Arthur Conan Doyle\", \"The adventures of Sherlock Holmes\", 1892,19.83),\n   (\"b00501\", \"Arthur Conan Doyle\", \"The memoirs of Sherlock Holmes\", 1893,14.22),\n   (\"b00300\", \"Arthur Conan Doyle\", \"The hounds of Baskerville\", 1901,12.25)\n).toDF(\"book_id\", \"book_author\", \"book_name\", \"book_pub_year\",\"book_price\")\n\n//Persist\nbooksDF.write.mode(\"append\").format(\"org.apache.spark.sql.cassandra\").options(Map( \"table\" -> \"books\", \"keyspace\" -> \"books_ks\", \"output.consistency.level\" -> \"ALL\", \"ttl\" -> \"10000000\")).save()", "outputs": [], "metadata": {"collapsed": true}}, {"source": "---\n## 3.0. Dataframe API", "cell_type": "markdown", "metadata": {}}, {"source": "### 3.0.1. Delete rows based on a condition", "cell_type": "markdown", "metadata": {}}, {"execution_count": 11, "cell_type": "code", "source": "//0) Review table data prior to delete operation\nspark.read.format(\"org.apache.spark.sql.cassandra\").options(Map( \"table\" -> \"books\", \"keyspace\" -> \"books_ks\")).load.show", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+-------+------------------+--------------------+----------+-------------+\n|book_id|       book_author|           book_name|book_price|book_pub_year|\n+-------+------------------+--------------------+----------+-------------+\n| b00300|Arthur Conan Doyle|The hounds of Bas...|     12.25|         1901|\n| b00001|Arthur Conan Doyle|  A study in scarlet|     11.33|         1887|\n| b00023|Arthur Conan Doyle|      A sign of four|     22.45|         1890|\n| b00501|Arthur Conan Doyle|The memoirs of Sh...|     14.22|         1893|\n| b01001|Arthur Conan Doyle|The adventures of...|     19.83|         1892|\n+-------+------------------+--------------------+----------+-------------+"}], "metadata": {"collapsed": false}}, {"execution_count": 12, "cell_type": "code", "source": "//1) Create dataframe based off of table\nval deleteBooksDF = spark.read.format(\"org.apache.spark.sql.cassandra\").options(Map( \"table\" -> \"books\", \"keyspace\" -> \"books_ks\")).load.filter(\"book_pub_year = 1887\")\n\n//2) Review execution plan\ndeleteBooksDF.explain", "outputs": [{"output_type": "stream", "name": "stdout", "text": "== Physical Plan ==\n*(1) Filter (isnotnull(book_pub_year#186) && (book_pub_year#186 = 1887))\n+- *(1) Scan org.apache.spark.sql.cassandra.CassandraSourceRelation@70a746c1 [book_id#182,book_author#183,book_name#184,book_price#185,book_pub_year#186] PushedFilters: [IsNotNull(book_pub_year), EqualTo(book_pub_year,1887)], ReadSchema: struct<book_id:string,book_author:string,book_name:string,book_price:float,book_pub_year:int>"}], "metadata": {"collapsed": false}}, {"execution_count": 13, "cell_type": "code", "source": "//3) Review rows to be deleted\ndeleteBooksDF.show", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+-------+------------------+------------------+----------+-------------+\n|book_id|       book_author|         book_name|book_price|book_pub_year|\n+-------+------------------+------------------+----------+-------------+\n| b00001|Arthur Conan Doyle|A study in scarlet|     11.33|         1887|\n+-------+------------------+------------------+----------+-------------+"}], "metadata": {"collapsed": false}}, {"execution_count": 14, "cell_type": "code", "source": "//4) Delete selected records in dataframe\n//Reuse connection for each partition\nval cdbConnector = CassandraConnector(sc)\ndeleteBooksDF.foreachPartition(partition => {\n  cdbConnector.withSessionDo(session =>\n    partition.foreach{ book => \n        val delete = s\"DELETE FROM books_ks.books where book_id='\"+book.getString(0) +\"';\"\n        session.execute(delete)\n    })\n})", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": 15, "cell_type": "code", "source": "//5) Review table data after delete operation\nspark.read.format(\"org.apache.spark.sql.cassandra\").options(Map( \"table\" -> \"books\", \"keyspace\" -> \"books_ks\")).load.show", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+-------+------------------+--------------------+----------+-------------+\n|book_id|       book_author|           book_name|book_price|book_pub_year|\n+-------+------------------+--------------------+----------+-------------+\n| b00300|Arthur Conan Doyle|The hounds of Bas...|     12.25|         1901|\n| b00023|Arthur Conan Doyle|      A sign of four|     22.45|         1890|\n| b00501|Arthur Conan Doyle|The memoirs of Sh...|     14.22|         1893|\n| b01001|Arthur Conan Doyle|The adventures of...|     19.83|         1892|\n+-------+------------------+--------------------+----------+-------------+"}], "metadata": {"collapsed": false}}, {"source": "### 3.0.2. Delete all rows in a table", "cell_type": "markdown", "metadata": {}}, {"execution_count": 18, "cell_type": "code", "source": "//1) Create dataframe\nval deleteBooksDF = spark.read.format(\"org.apache.spark.sql.cassandra\").options(Map( \"table\" -> \"books\", \"keyspace\" -> \"books_ks\")).load.select(\"book_id\")", "outputs": [{"output_type": "stream", "name": "stdout", "text": "deleteBooksDF: org.apache.spark.sql.DataFrame = [book_id: string]"}], "metadata": {"collapsed": false}}, {"execution_count": 19, "cell_type": "code", "source": "//2) Review partition keys to be deleted \ndeleteBooksDF.show", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+-------+\n|book_id|\n+-------+\n| b00300|\n| b00023|\n| b00501|\n| b01001|\n+-------+"}], "metadata": {"collapsed": false}}, {"execution_count": 20, "cell_type": "code", "source": "//3) Delete selected records in dataframe\n//Reuse connection for each partition\nval cdbConnector = CassandraConnector(sc)\ndeleteBooksDF.foreachPartition(partition => {\n  cdbConnector.withSessionDo(session =>\n    partition.foreach{ book => \n        val delete = s\"DELETE FROM books_ks.books where book_id='\"+book.getString(0) +\"';\"\n        session.execute(delete)\n    })\n})", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": 21, "cell_type": "code", "source": "//4) Review table data after delete operation - no rows should be returned\nspark.read.format(\"org.apache.spark.sql.cassandra\").options(Map( \"table\" -> \"books\", \"keyspace\" -> \"books_ks\")).load.show", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+-------+-----------+---------+----------+-------------+\n|book_id|book_author|book_name|book_price|book_pub_year|\n+-------+-----------+---------+----------+-------------+\n+-------+-----------+---------+----------+-------------+"}], "metadata": {"collapsed": false}}, {"source": "## 4.0. RDD API", "cell_type": "markdown", "metadata": {}}, {"source": "### 4.0.1. Delete rows based on a specific condition\nNot supported currently", "cell_type": "markdown", "metadata": {}}, {"source": "### 4.0.2. Delete all rows in a table\nRun the cell in 2.0 to generate some data", "cell_type": "markdown", "metadata": {}}, {"execution_count": 24, "cell_type": "code", "source": "//1) Create RDD with all rows in the table\nval deleteBooksRDD = sc.cassandraTable(\"books_ks\", \"books\").select(\"book_id\")", "outputs": [{"output_type": "stream", "name": "stdout", "text": "deleteBooksRDD: com.datastax.spark.connector.rdd.CassandraTableScanRDD[com.datastax.spark.connector.CassandraRow] = CassandraTableScanRDD[106] at RDD at CassandraRDD.scala:19"}], "metadata": {"collapsed": false}}, {"execution_count": 25, "cell_type": "code", "source": "//2) Review table data before execution\ndeleteBooksRDD.collect.foreach(println)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "CassandraRow{book_id: b00300}\nCassandraRow{book_id: b00001}\nCassandraRow{book_id: b00023}\nCassandraRow{book_id: b00501}\nCassandraRow{book_id: b01001}"}], "metadata": {"collapsed": false}}, {"execution_count": 26, "cell_type": "code", "source": "//3) Delete the targeted rows\n\n/* Option 1: \n// deleteFromCassandra is not supported\nsc.cassandraTable(\"books_ks\", \"books\")\n  .where(\"book_pub_year = 1891\")\n  .deleteFromCassandra(\"books_ks\", \"books\")\n*/\n\n//Option 2: CassandraConnector and CQL\n//Reuse connection for each partition\nval cdbConnector = CassandraConnector(sc)\ndeleteBooksRDD.foreachPartition(partition => {\n    cdbConnector.withSessionDo(session =>\n    partition.foreach{book => \n        val delete = s\"DELETE FROM books_ks.books where book_id='\"+ book.getString(0) +\"';\"\n        session.execute(delete)\n    }\n   )\n})", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": 28, "cell_type": "code", "source": "//4) Review table data after delete operation - no rows should be returned\nsc.cassandraTable(\"books_ks\", \"books\").collect.foreach(println)", "outputs": [], "metadata": {"collapsed": true}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Spark", "name": "sparkkernel", "language": ""}, "language_info": {"mimetype": "text/x-scala", "pygments_lexer": "scala", "name": "scala", "codemirror_mode": "text/x-scala"}}}